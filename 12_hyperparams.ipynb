{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L720dYTf8t0X",
        "outputId": "5b666728-1c20-4e8f-f128-5991e729369c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 45 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 53.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=fa8cb9bc1c74415f7950827a6539a8c9f79fa464a4302edbf30615b8fadab526\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"RDD\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "u0dtbL3W9upc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nILmBxOq8sWh"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Prepare training documents, which are labeled.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0),\n",
        "    (4, \"b spark who\", 1.0),\n",
        "    (5, \"g d a y\", 0.0),\n",
        "    (6, \"spark fly\", 1.0),\n",
        "    (7, \"was mapreduce\", 0.0),\n",
        "    (8, \"e spark program\", 1.0),\n",
        "    (9, \"a e c l\", 0.0),\n",
        "    (10, \"spark compile\", 1.0),\n",
        "    (11, \"hadoop software\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72IjDnHMCbDp",
        "outputId": "145bf2e2-6a95-47bf-b544-08e4a96e5113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-----+\n",
            "| id|            text|label|\n",
            "+---+----------------+-----+\n",
            "|  0| a b c d e spark|  1.0|\n",
            "|  1|             b d|  0.0|\n",
            "|  2|     spark f g h|  1.0|\n",
            "|  3|hadoop mapreduce|  0.0|\n",
            "|  4|     b spark who|  1.0|\n",
            "|  5|         g d a y|  0.0|\n",
            "|  6|       spark fly|  1.0|\n",
            "|  7|   was mapreduce|  0.0|\n",
            "|  8| e spark program|  1.0|\n",
            "|  9|         a e c l|  0.0|\n",
            "| 10|   spark compile|  1.0|\n",
            "| 11| hadoop software|  0.0|\n",
            "+---+----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
        "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
        "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
        "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
        "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=2)  # use 3+ folds in practice\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iFIBjWQ9CMGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat hasil tokenize\n",
        "tokenizer.transform(training).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVAXeyR_CkfJ",
        "outputId": "3d2ea3ae-1a36-4809-f453-9eadf9db9952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=0, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark']),\n",
              " Row(id=1, text='b d', label=0.0, words=['b', 'd']),\n",
              " Row(id=2, text='spark f g h', label=1.0, words=['spark', 'f', 'g', 'h']),\n",
              " Row(id=3, text='hadoop mapreduce', label=0.0, words=['hadoop', 'mapreduce']),\n",
              " Row(id=4, text='b spark who', label=1.0, words=['b', 'spark', 'who']),\n",
              " Row(id=5, text='g d a y', label=0.0, words=['g', 'd', 'a', 'y']),\n",
              " Row(id=6, text='spark fly', label=1.0, words=['spark', 'fly']),\n",
              " Row(id=7, text='was mapreduce', label=0.0, words=['was', 'mapreduce']),\n",
              " Row(id=8, text='e spark program', label=1.0, words=['e', 'spark', 'program']),\n",
              " Row(id=9, text='a e c l', label=0.0, words=['a', 'e', 'c', 'l'])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "\n",
        "# hashingTF.transform(tokenizer).head().features"
      ],
      "metadata": {
        "id": "nmsPFy9LDNU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashingTF.indexOf(\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1wu-XyFTBA",
        "outputId": "4a311f87-b58e-4974-adb1-93f5220911a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107107"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(training)"
      ],
      "metadata": {
        "id": "aYGZ0mEhCV6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel.explainParams()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "NNVuz-TwMlG4",
        "outputId": "89664f2d-6cce-430c-82ce-942170892af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"estimator: estimator to be cross-validated (current: Pipeline_99991ad73277)\\nestimatorParamMaps: estimator param maps (current: [{Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 10, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 100, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.1}, {Param(parent='HashingTF_ccadc3db79f3', name='numFeatures', doc='Number of features. Should be greater than 0.'): 1000, Param(parent='LogisticRegression_cd2ca802a668', name='regParam', doc='regularization parameter (>= 0).'): 0.01}])\\nevaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: BinaryClassificationEvaluator_4e4ff02e87d1)\\nfoldCol: Param for the column name of user specified fold number. Once this is specified, :py:class:`CrossValidator` won't do random k-fold split. Note that this column should be integer type with range [0, numFolds) and Spark will throw exception on out-of-range fold numbers. (default: )\\nnumFolds: number of folds for cross validation (default: 3, current: 2)\\nseed: random seed. (default: 4289139279920662944)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestModel = cvModel.bestModel\n",
        "bestModel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tOyL7ZrNilr",
        "outputId": "c718acc8-14b6-484a-a533-767aa004c372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PipelineModel_3396ab12237d"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test documents, which are unlabeled.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"mapreduce spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
        "prediction = cvModel.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf_euasBCWFA",
        "outputId": "54d93e30-7018-4512-ea86-49e4707dd195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(id=4, text='spark i j k', probability=DenseVector([0.0251, 0.9749]), prediction=1.0)\n",
            "Row(id=5, text='l m n', probability=DenseVector([0.8148, 0.1852]), prediction=0.0)\n",
            "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4425, 0.5575]), prediction=1.0)\n",
            "Row(id=7, text='apache hadoop', probability=DenseVector([0.764, 0.236]), prediction=0.0)\n"
          ]
        }
      ]
    }
  ]
}