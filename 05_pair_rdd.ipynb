{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDDs in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceBkey and collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Tampilkan output menggunakan iterasi\n",
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 5), (1, 2), (3, 10)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd_Reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortByKey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "# Urutkan reduced RDD\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=True)\n",
    "\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountingBykeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Hitung unique key\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD and transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Complete_Shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 128576\n"
     ]
    }
   ],
   "source": [
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah ke lowercase dan abaikan stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Buat tuple dari setiap kata\n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('EBook', 1)\n",
      "('Shakespeare', 12)\n",
      "('use', 38)\n",
      "('anyone', 1)\n",
      "('anywhere', 1)\n",
      "('restrictions', 1)\n",
      "('whatsoever.', 1)\n",
      "('may', 162)\n",
      "('it,', 74)\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Tukar key dan value\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou,650\n",
      "thy,574\n",
      "shall,393\n",
      "would,311\n",
      "good,295\n",
      "thee,286\n",
      "love,273\n",
      "Enter,269\n",
      "th',254\n",
      "make,225\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bc067213193d7d391907c464a8e66c6ece1a82666c1d2406921e2c8deb6cf3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
